[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hidden Markov Models (HMMs)",
    "section": "",
    "text": "Course modules at a glance\nWelcome to the Hidden Markov Models Course.\nThis site presents a calm, rigorous, graduate-level treatment of HMMs:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hidden Markov Models (HMMs)</span>"
    ]
  },
  {
    "objectID": "index.html#course-modules-at-a-glance",
    "href": "index.html#course-modules-at-a-glance",
    "title": "Hidden Markov Models (HMMs)",
    "section": "",
    "text": "Module A – Foundations (Sections 0–1)\nMathematical background for a rigorous HMM course:\n\nProbability spaces, conditional expectations, basic measure-theoretic language.\nStochastic matrices, Perron–Frobenius theory, spectral gap and mixing.\nFinite-state Markov chains: ergodicity, stationary laws, reversibility, and non-homogeneous chains.\n\n\n\nModule B – Building HMMs (Sections 2–3)\nConstruction of HMMs as probabilistic graphical models:\n\nConditional independence structure and joint factorization of \\(S_{1:T}, Y_{1:T}\\).\nObservation models: discrete, continuous, and exponential-family emissions.\nFormal HMM definition \\((\\boldsymbol{\\delta}, \\boldsymbol{\\Gamma}, f_1,\\dots,f_K)\\) and likelihood in matrix form.\n\n\n\nModule C – Inference Algorithms (Section 4)\nCore algorithms for posterior computation and decoding:\n\nForward filtering and numerically stable log / scaled implementations.\nForward–backward smoothing and pairwise state probabilities.\nViterbi decoding, dynamic programming optimality, and max-product semiring.\n\n\n\nModule D – Parameter Estimation & Identifiability (Section 5)\nHow to fit HMMs from data:\n\nMaximum likelihood estimation and non-convex log-likelihood geometry.\nEM / Baum–Welch as coordinate ascent on an evidence lower bound.\nIdentifiability up to label switching and structural pathologies.\n\n\n\nModule E – Statistical Theory & Advanced Models (Sections 6–9)\nAsymptotics and extensions beyond basic finite-state HMMs:\n\nConsistency and asymptotic normality of the MLE; Fisher information for dependent data.\nContinuous-state / state-space models and the Kalman filter as an HMM.\nNonparametric and infinite-state HMMs; online and decision-theoretic perspectives.\n\n\n\nModule F – Applications & Proof-Based Problems (Sections 10–11)\nConnecting theory to practice and consolidating understanding:\n\nApplications in speech recognition, bioinformatics, finance, epidemiology, and more.\nProof-based problem sets covering Markov chains, inference algorithms, EM, identifiability, and asymptotics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hidden Markov Models (HMMs)</span>"
    ]
  },
  {
    "objectID": "index.html#get-started-with-hmms-today",
    "href": "index.html#get-started-with-hmms-today",
    "title": "Hidden Markov Models (HMMs)",
    "section": "Get Started with HMMs Today!",
    "text": "Get Started with HMMs Today!\nStart with foundations View full curriculum\nUse the sidebar to jump directly to individual sections (0–11), or read them linearly as a graduate course. Mathematics is rendered directly in the browser, and all derivations are written to be compatible with the notation in Zucchini, MacDonald & Langrock and the more theoretical treatments of Capp'e–Moulines–Ryd'en and Douc–Moulines–Stoffer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hidden Markov Models (HMMs)</span>"
    ]
  },
  {
    "objectID": "section-0-mathematical-prerequisites/README.html",
    "href": "section-0-mathematical-prerequisites/README.html",
    "title": "Section 0 – Mathematical Prerequisites for Hidden Markov Models",
    "section": "",
    "text": "0.1 Measure-Theoretic Probability (Light but Precise)\nThis section collects the mathematical foundations required for a rigorous treatment of Hidden Markov Models (HMMs). The goal is not to teach full measure-theoretic probability from scratch, but to make precise the pieces that will be used repeatedly later.\nThroughout, we aim to be compatible with the notation and level of Zucchini, MacDonald, Langrock (“Zucchini et al.”) while pushing the theory somewhat further when needed for Sections 4–6.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 0 – Mathematical Prerequisites for Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "section-0-mathematical-prerequisites/README.html#measure-theoretic-probability-light-but-precise",
    "href": "section-0-mathematical-prerequisites/README.html#measure-theoretic-probability-light-but-precise",
    "title": "Section 0 – Mathematical Prerequisites for Hidden Markov Models",
    "section": "",
    "text": "0.1.1 Probability Spaces\nA probability space is a triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) where\n\n\\(\\Omega\\) is the sample space (set of outcomes);\n\\(\\mathcal{F} \\subseteq 2^{\\Omega}\\) is a \\(\\sigma\\)-algebra of events (closed under complements and countable unions);\n\\(\\mathbb{P} : \\mathcal{F} \\to [0,1]\\) is a probability measure with \\(\\mathbb{P}(\\Omega)=1\\) and countable additivity.\n\nFor HMMs we usually work with products of measurable spaces, e.g. sequences of states and observations. The relevant product \\(\\sigma\\)-algebras and measures are:\n\nFor a measurable space \\((S, \\mathcal{S})\\), the countable product \\((S^{\\mathbb{N}}, \\mathcal{S}^{\\otimes \\mathbb{N}})\\) is defined via the smallest \\(\\sigma\\)-algebra making all coordinate projections measurable.\nFor a Markov chain \\((S_t)_{t \\ge 1}\\), the joint law of the whole sequence lives on such a product space.\n\nIn finite-state HMMs, \\(S = \\{1,\\dots,K\\}\\) with the discrete \\(\\sigma\\)-algebra (all subsets), so measurability is trivial; nevertheless, the measure-theoretic formulation clarifies conditional expectations and ergodic theorems later.\n\n\n0.1.2 Random Variables and Distributions\nA random variable with values in a measurable space \\((S, \\mathcal{S})\\) is a measurable map \\[\nX : (\\Omega, \\mathcal{F}) \\to (S, \\mathcal{S}).\n\\]\nThe distribution (or law) of \\(X\\) is the pushforward measure \\(\\mathbb{P}_X\\) on \\((S, \\mathcal{S})\\): \\[\n\\mathbb{P}_X(A) = \\mathbb{P}(X \\in A), \\quad A \\in \\mathcal{S}.\n\\]\nIn HMMs we will consider random variables \\(S_t\\) (hidden states) and \\(Y_t\\) (observations). Their joint distribution factorizes in a special way due to the Markov property and conditional independence, which we will formalize later.\n\n\n0.1.3 Expectation and Conditional Expectation\nFor an integrable real-valued random variable \\(X\\), its expectation is \\[\n\\mathbb{E}[X] = \\int_{\\Omega} X(\\omega) \\, \\mathbb{P}(d\\omega),\n\\]\nor equivalently, if \\(X\\) takes values in \\(\\mathbb{R}\\) with distribution \\(\\mu = \\mathbb{P}_X\\), \\[\n\\mathbb{E}[X] = \\int_{\\mathbb{R}} x \\, \\mu(dx).\n\\]\nFor a sub-\\(\\sigma\\)-algebra \\(\\mathcal{G} \\subseteq \\mathcal{F}\\), the conditional expectation of \\(X\\) given \\(\\mathcal{G}\\) is a \\(\\mathcal{G}\\)-measurable random variable \\(\\mathbb{E}[X\\mid \\mathcal{G}]\\) such that \\[\n\\int_G \\mathbb{E}[X\\mid\\mathcal{G}] \\, d\\mathbb{P} = \\int_G X \\, d\\mathbb{P}, \\quad \\forall G \\in \\mathcal{G}.\n\\]\nKey properties (used constantly in HMM derivations):\n\nLinearity: \\(\\mathbb{E}[aX + bY \\mid \\mathcal{G}] = a\\,\\mathbb{E}[X\\mid\\mathcal{G}] + b\\,\\mathbb{E}[Y\\mid\\mathcal{G}]\\).\nTower property: If \\(\\mathcal{H} \\subseteq \\mathcal{G} \\subseteq \\mathcal{F}\\), then \\[\n\\mathbb{E}[\\mathbb{E}[X\\mid\\mathcal{G}]\\mid \\mathcal{H}] = \\mathbb{E}[X\\mid\\mathcal{H}].\n\\]\nTaking out what is known: If \\(Z\\) is \\(\\mathcal{G}\\)-measurable and integrable, \\[\n\\mathbb{E}[ZX\\mid\\mathcal{G}] = Z\\,\\mathbb{E}[X\\mid\\mathcal{G}].\n\\]\n\nIn HMMs, filtering and smoothing can be viewed as computing conditional expectations like \\(\\mathbb{E}[g(S_t) \\mid Y_{1:T}]\\) for suitable functions \\(g\\). The forward–backward algorithms are efficient implementations of these operations.\n\n\n0.1.4 Regular Conditional Probabilities\nGiven random variables \\(X\\) and \\(Y\\) on a probability space, a regular conditional probability of \\(X\\) given \\(Y=y\\) is a family of probability measures \\(\\{\\mathbb{P}(X \\in \\cdot \\mid Y=y)\\}\\) such that\n\nFor each measurable \\(A\\), the map \\(y \\mapsto \\mathbb{P}(X \\in A \\mid Y=y)\\) is measurable;\nFor each measurable \\(B\\), \\[\n\\mathbb{P}(X \\in B, Y \\in C) = \\int_C \\mathbb{P}(X \\in B \\mid Y=y) \\, \\mathbb{P}_Y(dy).\n\\]\n\nOn standard Borel spaces (Polish spaces with their Borel \\(\\sigma\\)-algebra), regular conditional probabilities always exist and are unique up to \\(\\mathbb{P}_Y\\)-null sets. This justifies writing objects like \\[\n\\mathbb{P}(S_t = i \\mid Y_{1:T}=y_{1:T})\n\\] rigorously, which is what the forward–backward algorithms compute.\n\n\n0.1.5 Modes of Convergence\nWe briefly recall three notions of convergence for a sequence of random variables \\((X_n)\\):\n\nAlmost sure (a.s.) convergence: \\(X_n \\to X\\) a.s. if \\[\n\\mathbb{P}\\bigl(\\{\\omega : X_n(\\omega) \\to X(\\omega)\\}\\bigr) = 1.\n\\]\nConvergence in probability: \\(X_n \\to X\\) in probability if, for all \\(\\varepsilon &gt; 0\\), \\[\n\\lim_{n\\to\\infty} \\mathbb{P}(|X_n - X| &gt; \\varepsilon) = 0.\n\\]\n\\(L^p\\) convergence: \\(X_n \\to X\\) in \\(L^p\\) (for \\(p \\ge 1\\)) if \\[\n\\lim_{n\\to\\infty} \\mathbb{E}[|X_n - X|^p] = 0.\n\\]\n\nFor asymptotic theory in HMMs (Section 6), we will need laws of large numbers and central limit theorems for functionals of an ergodic Markov chain. These are typically stated in terms of convergence in probability or distribution, and proved using almost sure convergence plus dominated convergence.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 0 – Mathematical Prerequisites for Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "section-0-mathematical-prerequisites/README.html#linear-algebra-and-spectral-theory",
    "href": "section-0-mathematical-prerequisites/README.html#linear-algebra-and-spectral-theory",
    "title": "Section 0 – Mathematical Prerequisites for Hidden Markov Models",
    "section": "0.2 Linear Algebra and Spectral Theory",
    "text": "0.2 Linear Algebra and Spectral Theory\n\n0.2.1 Probability Vectors and the Simplex\nFor a finite state space of size \\(K\\), a probability vector is \\[\n\\boldsymbol{\\mu} = (\\mu_1, \\dots, \\mu_K)^\\top, \\quad \\mu_i \\ge 0, \\quad \\sum_{i=1}^K \\mu_i = 1.\n\\]\nThe set of all such vectors is the probability simplex \\[\n\\Delta^{K-1} = \\Bigl\\{ \\boldsymbol{\\mu} \\in \\mathbb{R}^K : \\mu_i \\ge 0, \\sum_i \\mu_i = 1 \\Bigr\\}.\n\\]\nWe measure distances on \\(\\Delta^{K-1}\\) using norms:\n\n\\(\\ell^1\\) norm: \\(\\lVert \\mu - \\nu \\rVert_1 = \\sum_i |\\mu_i - \\nu_i|\\) (twice the total variation distance);\n\\(\\ell^2\\) norm: \\(\\lVert \\mu - \\nu \\rVert_2 = (\\sum_i (\\mu_i - \\nu_i)^2)^{1/2}\\).\n\nBoth will appear in mixing-time and stability results for Markov chains and filters.\n\n\n0.2.2 Stochastic Matrices\nA row-stochastic matrix is a \\(K \\times K\\) matrix \\(\\boldsymbol{\\Gamma} = (\\gamma_{ij})\\) with \\[\n\\gamma_{ij} \\ge 0, \\quad \\sum_{j=1}^K \\gamma_{ij} = 1 \\quad \\text{for all } i.\n\\]\nIn finite-state HMMs (following Zucchini et al.), \\(\\boldsymbol{\\Gamma}\\) denotes the transition matrix of the hidden Markov chain \\((S_t)\\): \\[\n\\gamma_{ij} = \\mathbb{P}(S_{t+1} = j \\mid S_t = i).\n\\]\nGiven a probability vector \\(\\boldsymbol{\\mu}\\), the product \\(\\boldsymbol{\\mu}^\\top \\boldsymbol{\\Gamma}\\) is again a probability vector, representing the distribution of \\(S_{t+1}\\) if \\(\\boldsymbol{\\mu}\\) is the distribution of \\(S_t\\).\n\n\n0.2.3 Perron–Frobenius Theory\nFor a non-negative matrix \\(A \\in \\mathbb{R}^{K \\times K}\\) (i.e. \\(A_{ij} \\ge 0\\)), the Perron–Frobenius theorem gives powerful spectral properties. In particular, if \\(A\\) is irreducible, then\n\nThere exists a positive eigenvalue \\(\\rho(A) &gt; 0\\) (the spectral radius) with a corresponding positive eigenvector \\(v &gt; 0\\).\n\\(\\rho(A)\\) is simple (algebraic multiplicity 1), and no other eigenvector with non-negative entries exists for a different eigenvalue.\n\nFor a stochastic matrix \\(\\boldsymbol{\\Gamma}\\):\n\nIts spectral radius satisfies \\(\\rho(\\boldsymbol{\\Gamma}) = 1\\), since \\(\\boldsymbol{\\Gamma}\\mathbf{1} = \\mathbf{1}\\).\nIf \\(\\boldsymbol{\\Gamma}\\) is irreducible and aperiodic, the left eigenvector corresponding to eigenvalue 1, normalized to sum to 1, is the unique stationary distribution \\(\\boldsymbol{\\pi}\\): \\[\n\\boldsymbol{\\pi}^\\top \\boldsymbol{\\Gamma} = \\boldsymbol{\\pi}^\\top.\n\\]\n\nThis provides the spectral foundation for ergodicity of finite-state Markov chains, and later for stability of HMM filters.\n\n\n0.2.4 Spectral Gap and Convergence Rates\nLet the eigenvalues of a stochastic matrix \\(\\boldsymbol{\\Gamma}\\) be ordered as \\[\n1 = \\lambda_1 &gt; |\\lambda_2| \\ge \\dots \\ge |\\lambda_K|.\n\\]\nThe spectral gap is \\[\n\\gamma := 1 - |\\lambda_2|.\n\\]\nFor many chains (especially reversible ones), the convergence of \\(\\boldsymbol{\\mu}_0^\\top \\boldsymbol{\\Gamma}^t\\) to the stationary distribution \\(\\boldsymbol{\\pi}^\\top\\) in \\(\\ell^2\\) or total variation can be bounded in terms of \\(\\gamma\\). Roughly, \\[\n\\lVert \\boldsymbol{\\mu}_0^\\top \\boldsymbol{\\Gamma}^t - \\boldsymbol{\\pi}^\\top \\rVert_2 \\le C (1-\\gamma)^t.\n\\]\nMore precise inequalities follow from the spectral decomposition of \\(\\boldsymbol{\\Gamma}\\) and, in the reversible case, from its self-adjointness in \\(L^2(\\boldsymbol{\\pi})\\).\nThese ideas will underpin mixing-time and filter stability results (Sections 1.2 and 4.1).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 0 – Mathematical Prerequisites for Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "section-0-mathematical-prerequisites/README.html#optimization-and-information-geometry",
    "href": "section-0-mathematical-prerequisites/README.html#optimization-and-information-geometry",
    "title": "Section 0 – Mathematical Prerequisites for Hidden Markov Models",
    "section": "0.3 Optimization and Information Geometry",
    "text": "0.3 Optimization and Information Geometry\n\n0.3.1 Convexity on the Probability Simplex\nA function \\(f : \\Delta^{K-1} \\to \\mathbb{R}\\) is convex if \\[\nf(\\theta \\mu + (1-\\theta)\\nu) \\le \\theta f(\\mu) + (1-\\theta) f(\\nu)\n\\] for all \\(\\mu, \\nu \\in \\Delta^{K-1}\\) and \\(\\theta \\in [0,1]\\).\nMany information-theoretic functionals are convex or strictly convex on \\(\\Delta^{K-1}\\). Examples:\n\nNegative entropy \\(H(\\mu) = -\\sum_i \\mu_i \\log \\mu_i\\) is strictly concave;\nThe Kullback–Leibler divergence (KL) is jointly convex in \\((p,q)\\).\n\nConvexity is central in understanding EM updates, variational approximations, and the geometry of the log-likelihood surface in HMMs.\n\n\n0.3.2 Kullback–Leibler Divergence as a Bregman Divergence\nFor two discrete distributions \\(p,q \\in \\Delta^{K-1}\\) with full support (\\(p_i, q_i &gt; 0\\)), the KL divergence is \\[\n\\mathrm{KL}(p \\Vert q) = \\sum_{i=1}^K p_i \\log\\frac{p_i}{q_i}.\n\\]\nKL divergence can be written as a Bregman divergence associated with the negative entropy function \\[\n\\phi(p) = \\sum_i p_i \\log p_i.\n\\]\nThe Bregman divergence generated by \\(\\phi\\) is \\[\nD_\\phi(p,q) = \\phi(p) - \\phi(q) - \\langle \\nabla \\phi(q), p - q \\rangle,\n\\] where \\(\\langle \\cdot,\\cdot \\rangle\\) is the usual inner product on \\(\\mathbb{R}^K\\). A straightforward calculation shows \\[\nD_\\phi(p,q) = \\mathrm{KL}(p \\Vert q).\n\\]\nThis interpretation highlights several facts:\n\n\\(\\mathrm{KL}(p \\Vert q) \\ge 0\\) with equality iff \\(p=q\\) (strict convexity of \\(\\phi\\));\nKL is asymmetric, unlike a metric, which shapes the geometry of likelihood-based optimization.\n\nIn HMMs, KL divergence arises when analyzing consistency and information projections, and in understanding why the EM algorithm can be seen as coordinate ascent on a lower bound involving KL terms.\n\n\n0.3.3 Duality and Entropy-Regularized Problems\nGiven a convex function \\(\\phi\\), its convex conjugate \\(\\phi^*\\) is \\[\n\\phi^*(y) = \\sup_{x} \\{ \\langle x, y \\rangle - \\phi(x) \\}.\n\\]\nFor \\(\\phi(p) = \\sum_i p_i \\log p_i\\) (negative entropy), \\(\\phi^*\\) is the log-partition function \\[\n\\phi^*(\\eta) = \\log \\sum_i e^{\\eta_i}.\n\\]\nThis duality underlies the exponential family structure of many emission distributions (Section 2.2) and appears in variational formulations of inference in HMMs:\n\nEntropy-regularized objectives of the form \\[\n\\max_{q} \\Big\\{ \\mathbb{E}_q[\\log p(Y,S)] + H(q) \\Big\\}\n\\] lead to exponential-family solutions for the optimal \\(q\\).\n\nIn the context of Zucchini et al., this background explains why log-sum-exp expressions appear in marginal likelihoods and why certain optimization problems have tractable, closed-form updates.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 0 – Mathematical Prerequisites for Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "section-0-mathematical-prerequisites/README.html#summary-and-connection-to-later-sections",
    "href": "section-0-mathematical-prerequisites/README.html#summary-and-connection-to-later-sections",
    "title": "Section 0 – Mathematical Prerequisites for Hidden Markov Models",
    "section": "0.4 Summary and Connection to Later Sections",
    "text": "0.4 Summary and Connection to Later Sections\nAfter this section, you should be comfortable with:\n\nProbability spaces, random variables, and conditional expectations in a measure-theoretic language;\nFinite-dimensional linear algebra for stochastic matrices, including Perron–Frobenius theory and spectral gaps;\nBasic convex analysis on probability simplices, and the interpretation of KL divergence as a Bregman divergence.\n\nThese tools will be used heavily in:\n\nSection 1: rigorous Markov chain theory (ergodicity, mixing);\nSection 3–4: derivation and correctness proofs of forward–backward and Viterbi algorithms;\nSection 5–6: EM algorithm analysis, identifiability, consistency, and asymptotic normality.\n\nFor a softer introduction, you may cross-reference:\n\nZucchini et al., Chapters 1–2, for probabilistic notation and basic Markov chain ideas;\nMurphy (2012), Chapters 2–3, for probability and exponential families;\nCappé, Moulines, Rydén (2005), Chapter 1, for a more advanced measure-theoretic setup.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 0 – Mathematical Prerequisites for Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "section-1-markov-chains/README.html",
    "href": "section-1-markov-chains/README.html",
    "title": "Section 1 – Markov Chains (Fully Rigorous)",
    "section": "",
    "text": "1.1 Finite-State Markov Chains\nThis section develops the Markov chain theory that underlies finite-state HMMs. We focus on:\nZucchini et al. treat finite-state Markov chains at an applied level; here we give a more rigorous account compatible with their notation.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Section 1 – Markov Chains (Fully Rigorous)</span>"
    ]
  },
  {
    "objectID": "section-1-markov-chains/README.html#finite-state-markov-chains",
    "href": "section-1-markov-chains/README.html#finite-state-markov-chains",
    "title": "Section 1 – Markov Chains (Fully Rigorous)",
    "section": "",
    "text": "1.1.1 Definition and Transition Kernels\nLet the state space be \\(E = \\{1,\\dots,K\\}\\). A stochastic process \\((S_t)_{t \\ge 1}\\) with values in \\(E\\) is a (time-homogeneous) Markov chain with transition matrix \\(\\boldsymbol{\\Gamma} = (\\gamma_{ij})\\) if \\[\n\\mathbb{P}(S_{t+1} = j \\mid S_1, \\dots, S_t) = \\mathbb{P}(S_{t+1} = j \\mid S_t) = \\gamma_{S_t j}, \\quad \\forall t \\ge 1.\n\\]\nEquivalently, for any sequence \\(i_1, \\dots, i_T\\) in \\(E\\), the joint probability is \\[\n\\mathbb{P}(S_1 = i_1, \\dots, S_T = i_T)\n= \\delta_{i_1} \\prod_{t=1}^{T-1} \\gamma_{i_t i_{t+1}},\n\\] where \\(\\boldsymbol{\\delta} = (\\delta_i)\\) is the initial distribution \\(\\delta_i = \\mathbb{P}(S_1 = i)\\).\nThis is exactly the hidden-state dynamics that Zucchini et al. use to define finite-state HMMs; the HMM adds an observation process on top of this chain.\n\n\n1.1.2 Chapman–Kolmogorov Equations\nLet \\(\\boldsymbol{\\Gamma}^{(n)}\\) denote the \\(n\\)-step transition matrix, with entries \\[\n\\gamma^{(n)}_{ij} = \\mathbb{P}(S_{t+n} = j \\mid S_t = i).\n\\]\nThen the Chapman–Kolmogorov equations state that for all \\(m,n \\ge 0\\), \\[\n\\boldsymbol{\\Gamma}^{(m+n)} = \\boldsymbol{\\Gamma}^{(m)} \\boldsymbol{\\Gamma}^{(n)}.\n\\]\nIn particular, \\(\\boldsymbol{\\Gamma}^{(n)} = \\boldsymbol{\\Gamma}^n\\) (the usual matrix power). This ties Markov chain evolution directly to the spectral properties of \\(\\boldsymbol{\\Gamma}\\).\n\n\n1.1.3 Stationary and Invariant Distributions\nA probability vector \\(\\boldsymbol{\\pi} \\in \\Delta^{K-1}\\) is a stationary distribution for \\(\\boldsymbol{\\Gamma}\\) if \\[\n\\boldsymbol{\\pi}^\\top \\boldsymbol{\\Gamma} = \\boldsymbol{\\pi}^\\top.\n\\]\nInterpretation:\n\nIf \\(S_1 \\sim \\boldsymbol{\\pi}\\), then \\(S_t \\sim \\boldsymbol{\\pi}\\) for all \\(t\\); the chain is in equilibrium.\nIf the chain is irreducible and aperiodic, \\(\\boldsymbol{\\pi}\\) is unique, and the distribution of \\(S_t\\) converges to \\(\\boldsymbol{\\pi}\\) for any initial distribution \\(\\boldsymbol{\\delta}\\).\n\nThe existence and uniqueness of \\(\\boldsymbol{\\pi}\\) are guaranteed by Perron–Frobenius theory (Section 0.2) for irreducible, aperiodic stochastic matrices.\n\n\n1.1.4 Reversibility and Detailed Balance\nA Markov chain with transition matrix \\(\\boldsymbol{\\Gamma}\\) and stationary distribution \\(\\boldsymbol{\\pi}\\) is reversible if it satisfies the detailed balance equations \\[\n\\pi_i \\, \\gamma_{ij} = \\pi_j \\, \\gamma_{ji}, \\quad \\forall i,j.\n\\]\nIntuitively, under stationarity, the probability flow from \\(i\\) to \\(j\\) equals that from \\(j\\) to \\(i\\).\nConsequences:\n\nIn the inner product space \\(L^2(\\boldsymbol{\\pi})\\), \\(\\boldsymbol{\\Gamma}\\) is self-adjoint: \\[\n\\langle f, \\boldsymbol{\\Gamma} g \\rangle_\\pi = \\langle \\boldsymbol{\\Gamma} f, g \\rangle_\\pi\n\\] for functions \\(f,g : E \\to \\mathbb{R}\\), where \\(\\langle f,g \\rangle_\\pi = \\sum_i \\pi_i f(i) g(i)\\).\nHence, the spectrum of \\(\\boldsymbol{\\Gamma}\\) is real, and spectral analysis is particularly transparent.\n\nIn HMMs, even if the hidden chain is not assumed reversible, reversible chains are a useful class for examples, counterexamples, and mixing-time calculations.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Section 1 – Markov Chains (Fully Rigorous)</span>"
    ]
  },
  {
    "objectID": "section-1-markov-chains/README.html#ergodic-theory-of-markov-chains",
    "href": "section-1-markov-chains/README.html#ergodic-theory-of-markov-chains",
    "title": "Section 1 – Markov Chains (Fully Rigorous)",
    "section": "1.2 Ergodic Theory of Markov Chains",
    "text": "1.2 Ergodic Theory of Markov Chains\n\n1.2.1 Irreducibility and Communication Classes\nFor states \\(i,j \\in E\\), write \\(i \\rightsquigarrow j\\) if there exists \\(n \\ge 0\\) such that \\(\\gamma^{(n)}_{ij} &gt; 0\\) (a path of positive probability from \\(i\\) to \\(j\\)). We say \\(i\\) communicates with \\(j\\), written \\(i \\leftrightarrow j\\), if both \\(i \\rightsquigarrow j\\) and \\(j \\rightsquigarrow i\\) hold.\nThis is an equivalence relation, partitioning \\(E\\) into communicating classes. A chain is irreducible if it has a single communicating class (every state communicates with every other).\nIn HMMs, irreducibility of the hidden chain ensures that every state can eventually be reached from any other, which is important for:\n\nExistence and uniqueness of a stationary distribution;\nIdentifiability and mixing assumptions in asymptotic theory (Section 6).\n\n\n\n1.2.2 Periodicity and Aperiodicity\nThe period of a state \\(i\\) is \\[\n\\mathrm{per}(i) = \\gcd\\{ n \\ge 1 : \\gamma^{(n)}_{ii} &gt; 0 \\}.\n\\]\nIn an irreducible chain, all states share the same period, so we can speak of the period of the chain. A chain is aperiodic if \\(\\mathrm{per}(i) = 1\\) for some (hence all) \\(i\\).\nAperiodicity rules out deterministic cycles and is necessary for convergence of \\(\\mathbb{P}(S_t = \\cdot)\\) to \\(\\boldsymbol{\\pi}\\) in total variation.\n\n\n1.2.3 Ergodic Theorem for Finite-State Markov Chains\nLet \\((S_t)\\) be irreducible and aperiodic with stationary distribution \\(\\boldsymbol{\\pi}\\). Then for any bounded function \\(f : E \\to \\mathbb{R}\\), \\[\n\\frac{1}{T} \\sum_{t=1}^T f(S_t) \\xrightarrow[T\\to\\infty]{\\text{a.s.}} \\sum_{i=1}^K \\pi_i f(i) =: \\mathbb{E}_\\pi[f(S)].\n\\]\nThis is the ergodic theorem: time averages converge almost surely to space averages under \\(\\boldsymbol{\\pi}\\). It is a Markov-chain version of the strong law of large numbers.\nIn HMMs, ergodic theorems are used to prove consistency of estimators and to analyze limiting behavior of likelihoods per unit time.\n\n\n1.2.4 Mixing Times and Total Variation Distance\nFor a probability vector \\(\\boldsymbol{\\mu}\\) on \\(E\\), the total variation distance to \\(\\boldsymbol{\\pi}\\) is \\[\n\\lVert \\boldsymbol{\\mu} - \\boldsymbol{\\pi} \\rVert_{\\mathrm{TV}}\n= \\frac{1}{2} \\sum_{i=1}^K |\\mu_i - \\pi_i|.\n\\]\nLet \\(\\boldsymbol{\\mu}_t = \\boldsymbol{\\delta}^\\top \\boldsymbol{\\Gamma}^t\\) be the distribution of \\(S_t\\) starting from \\(\\boldsymbol{\\delta}\\). The mixing time \\(t_{\\mathrm{mix}}(\\varepsilon)\\) is \\[\nt_{\\mathrm{mix}}(\\varepsilon) = \\min\\Bigl\\{ t : \\sup_{\\boldsymbol{\\delta}} \\lVert \\boldsymbol{\\mu}_t - \\boldsymbol{\\pi} \\rVert_{\\mathrm{TV}} \\le \\varepsilon \\Bigr\\}.\n\\]\nIn finite-state irreducible aperiodic chains, \\(t_{\\mathrm{mix}}(\\varepsilon) &lt; \\infty\\) for all \\(\\varepsilon &gt; 0\\). Spectral methods and coupling (next subsection) give quantitative bounds.\n\n\n1.2.5 Spectral Gap and Convergence Rates\nSuppose the chain is reversible with respect to \\(\\boldsymbol{\\pi}\\), with eigenvalues of \\(\\boldsymbol{\\Gamma}\\) ordered as \\[\n1 = \\lambda_1 &gt; \\lambda_2 \\ge \\dots \\ge \\lambda_K &gt; -1.\n\\]\nThe spectral gap is \\(\\gamma = 1 - \\lambda_2\\). One can show (see e.g. books on Markov chain mixing) that \\[\n\\lVert \\boldsymbol{\\mu}_t - \\boldsymbol{\\pi} \\rVert_{\\mathrm{TV}}\n\\le C \\, (1-\\gamma)^t\n\\] for some constant \\(C\\) depending on \\(\\boldsymbol{\\delta}\\). Thus, a larger spectral gap implies faster convergence to stationarity.\nIn HMMs, these spectral-gap-based bounds transfer to stability of the filtering distribution: the distribution of \\(S_t\\) given observations becomes asymptotically independent of the initial distribution.\n\n\n1.2.6 Coupling Arguments (Sketch)\nA powerful probabilistic technique for bounding mixing times is coupling: construct two copies of the chain, \\((S_t)\\) and \\((S'_t)\\), possibly dependent, such that\n\nMarginally, each evolves according to \\(\\boldsymbol{\\Gamma}\\);\nThey eventually coalesce: \\(S_t = S'_t\\) for all sufficiently large \\(t\\).\n\nDefine the coupling time \\[\nT_c = \\inf\\{ t \\ge 0 : S_t = S'_t \\}.\n\\]\nThen for any initial distributions \\(\\boldsymbol{\\delta}, \\boldsymbol{\\delta}'\\), \\[\n\\lVert \\boldsymbol{\\mu}_t - \\boldsymbol{\\mu}'_t \\rVert_{\\mathrm{TV}}\n\\le \\mathbb{P}(T_c &gt; t).\n\\]\nHence, controlling \\(\\mathbb{P}(T_c &gt; t)\\) yields mixing bounds. The idea of coupling will reappear implicitly in filter stability results in HMMs.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Section 1 – Markov Chains (Fully Rigorous)</span>"
    ]
  },
  {
    "objectID": "section-1-markov-chains/README.html#non-homogeneous-markov-chains",
    "href": "section-1-markov-chains/README.html#non-homogeneous-markov-chains",
    "title": "Section 1 – Markov Chains (Fully Rigorous)",
    "section": "1.3 Non-Homogeneous Markov Chains",
    "text": "1.3 Non-Homogeneous Markov Chains\nIn some extensions of HMMs, the hidden state process may have time-varying transitions, represented by a sequence of stochastic matrices \\((\\boldsymbol{\\Gamma}_t)\\). Then \\[\n\\mathbb{P}(S_{t+1} = j \\mid S_t = i) = (\\boldsymbol{\\Gamma}_t)_{ij}.\n\\]\n\n1.3.1 Product of Time-Varying Kernels\nDefine the \\(n\\)-step transition kernel from time \\(t\\) to \\(t+n\\) as \\[\n\\boldsymbol{\\Gamma}_{t, t+n} = \\boldsymbol{\\Gamma}_t \\boldsymbol{\\Gamma}_{t+1} \\cdots \\boldsymbol{\\Gamma}_{t+n-1}.\n\\]\nThe analog of Chapman–Kolmogorov holds in the obvious way: \\[\n\\boldsymbol{\\Gamma}_{t, t+m+n} = \\boldsymbol{\\Gamma}_{t, t+m} \\boldsymbol{\\Gamma}_{t+m, t+m+n}.\n\\]\n\n\n1.3.2 Stability Conditions\nWithout time-homogeneity, there may be no stationary distribution. Instead, one studies stability and ergodicity via conditions such as:\n\nUniform Doeblin conditions (lower bounds on transition probabilities);\nDobrushin contraction coefficients ensuring that products of kernels contract distances between probability distributions.\n\nThese ideas become particularly relevant when considering non-stationary HMMs or online learning settings (see Section 9).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Section 1 – Markov Chains (Fully Rigorous)</span>"
    ]
  },
  {
    "objectID": "section-1-markov-chains/README.html#connection-to-hmms-and-zucchini-et-al.",
    "href": "section-1-markov-chains/README.html#connection-to-hmms-and-zucchini-et-al.",
    "title": "Section 1 – Markov Chains (Fully Rigorous)",
    "section": "1.4 Connection to HMMs and Zucchini et al.",
    "text": "1.4 Connection to HMMs and Zucchini et al.\nIn Zucchini et al., the hidden process \\((S_t)\\) of an HMM is always a finite-state Markov chain with transition matrix \\(\\boldsymbol{\\Gamma}\\) and initial distribution \\(\\boldsymbol{\\delta}\\). The properties introduced here feed directly into later sections:\n\nSection 3: Uses the Markov property to factorize the joint HMM likelihood;\nSection 4: Forward–backward and Viterbi algorithms exploit \\(\\boldsymbol{\\Gamma}\\) as the transition kernel;\nSection 6: Ergodicity and mixing of \\((S_t)\\) underpin consistency and CLTs for estimators.\n\nFor more detailed Markov chain theory in a measure-theoretic style, see:\n\nCappé, Moulines, Rydén (2005), Chapters 1–2;\nDouc, Moulines, Stoffer (2014), Chapters 2–3.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Section 1 – Markov Chains (Fully Rigorous)</span>"
    ]
  }
]